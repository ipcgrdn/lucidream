"use client";

import { useState, useRef, useCallback } from "react";

interface UseSpeechRecognitionProps {
  onTranscriptionComplete: (text: string) => void;
  onError?: (error: string) => void;
}

export const useSpeechRecognition = ({
  onTranscriptionComplete,
  onError,
}: UseSpeechRecognitionProps) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const monitoringRef = useRef<boolean>(false);
  const speechDetectedRef = useRef<boolean>(false);

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜
  const monitorAudioLevel = useCallback(() => {
    if (!analyserRef.current || !monitoringRef.current) return;

    const bufferLength = analyserRef.current.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    analyserRef.current.getByteFrequencyData(dataArray);

    // í‰ê·  ìŒì„± ë ˆë²¨ ê³„ì‚°
    const average =
      dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;

    const SPEECH_THRESHOLD = 25; // ìŒì„± ê°ì§€ ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
    const SILENCE_THRESHOLD = 15; // ë¬´ìŒ ê°ì§€ ì„ê³„ê°’
    const SILENCE_DURATION = 2000; // 2ì´ˆê°„ ë¬´ìŒ ì‹œ ì¢…ë£Œ

    if (average > SPEECH_THRESHOLD) {
      // ìŒì„± ê°ì§€ë¨
      if (!speechDetectedRef.current) {
        console.log("ğŸ¤ Speech detected! Starting recording...");
        speechDetectedRef.current = true;
        setIsRecording(true);

        // MediaRecorder ì‹œì‘
        if (mediaRecorderRef.current?.state === "inactive") {
          mediaRecorderRef.current.start();
        }
      }

      // ë¬´ìŒ íƒ€ì´ë¨¸ ë¦¬ì…‹
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
    } else if (average < SILENCE_THRESHOLD && speechDetectedRef.current) {
      // ë¬´ìŒ ê°ì§€ë¨ (ì´ë¯¸ ìŒì„±ì´ ê°ì§€ëœ í›„)
      if (!silenceTimerRef.current) {
        silenceTimerRef.current = setTimeout(() => {
          console.log("ğŸ”‡ Silence detected! Stopping recording...");
          stopRecording();
        }, SILENCE_DURATION);
      }
    }

    // ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ë‹¤ì‹œ ëª¨ë‹ˆí„°ë§
    if (monitoringRef.current) {
      requestAnimationFrame(monitorAudioLevel);
    }
  }, []);

  const startRecording = useCallback(async () => {
    try {
      console.log("ğŸ™ï¸ Initializing voice detection...");

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      // Audio Context ì„¤ì •
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;

      // ë§ˆì´í¬ ì…ë ¥ì„ Audio Contextì— ì—°ê²°
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.3;

      source.connect(analyser);
      analyserRef.current = analyser;

      // MediaRecorder ì„¤ì • (ìë™ìœ¼ë¡œ ì‹œì‘ë˜ì§€ ì•ŠìŒ)
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: "audio/webm;codecs=opus",
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      // ë°ì´í„° ìˆ˜ì§‘
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      // ë…¹ìŒ ì™„ë£Œ ì²˜ë¦¬
      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, {
          type: "audio/webm;codecs=opus",
        });

        console.log("ğŸ“¦ Processing recorded audio...");
        // Whisper API í˜¸ì¶œ
        await sendToWhisper(audioBlob);
      };

      // ìƒíƒœ ì´ˆê¸°í™”
      speechDetectedRef.current = false;
      monitoringRef.current = true;

      // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
      console.log("ğŸ‘‚ Listening for speech...");
      monitorAudioLevel();
    } catch (error) {
      console.error("Failed to start recording:", error);
      onError?.("ë§ˆì´í¬ ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.");
    }
  }, [onError, monitorAudioLevel]);

  const stopRecording = useCallback(() => {
    console.log("ğŸ›‘ Stopping voice detection...");

    // ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
    monitoringRef.current = false;
    speechDetectedRef.current = false;

    // ë¬´ìŒ íƒ€ì´ë¨¸ ì •ë¦¬
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    // MediaRecorder ì¤‘ì§€ (ë…¹ìŒ ì¤‘ì¸ ê²½ìš°ë§Œ)
    if (
      mediaRecorderRef.current &&
      mediaRecorderRef.current.state === "recording"
    ) {
      mediaRecorderRef.current.stop();
      setIsProcessing(true);
    }

    // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    // Audio Context ì •ë¦¬
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    setIsRecording(false);
    analyserRef.current = null;
  }, []);

  const sendToWhisper = async (audioBlob: Blob) => {
    try {
      // FormData ìƒì„±
      const formData = new FormData();
      formData.append("audio", audioBlob, "recording.webm");

      // Whisper API í˜¸ì¶œ
      const response = await fetch("/api/whisper", {
        method: "POST",
        body: formData,
      });

      if (!response.ok) {
        throw new Error("ìŒì„± ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.");
      }

      const data = await response.json();

      if (data.success && data.text) {
        onTranscriptionComplete(data.text);
      } else {
        throw new Error("ìŒì„±ì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.");
      }
    } catch (error) {
      console.error("Whisper API error:", error);
      onError?.("ìŒì„± ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
    } finally {
      setIsProcessing(false);
    }
  };

  return {
    isRecording,
    isProcessing,
    startRecording,
    stopRecording,
  };
};
